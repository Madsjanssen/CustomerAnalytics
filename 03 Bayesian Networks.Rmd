---
title: "03 Bayesian Networks"
author: "Morten Gade"
date: "2024-05-30"
output: html_document
---

# Packages and data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 0)
library(dplyr)
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(gRbase)
library (caTools)
library(poLCA)
retention <- read.csv("../Code/Data/retention.csv", header = T, colClasses = "factor" )
retention_test <- read.csv("../Code/Data/retention_test.csv", header = T, colClasses = "factor" )
```

# Creating DAGs

DAG: Directed Acyclical Graph
- No loops
- Direction: Causality

We go through two methods for creating DAGs here:
1. Building the structure manually and introducing probabilities manually 
2. Learning the structure and the probabilities from data - most common

## Build manually

First, we define the DAG.
There are several ways of doing this, I have included 3 options below.
Then, we need to specify probabilities and conditional probabilities.

### Build DAG

```{r Method 1}
# Create an empty graph 
dag1 <- empty.graph(nodes = c("Fuse","Plea","Atti","Comm"))

# Add the arcs that encode the direct dependencies between variables
dag1 <- set.arc (dag1, from = "Fuse", to = "Atti")
dag1 <- set.arc (dag1, from = "Plea", to = "Atti")
dag1 <- set.arc (dag1, from = "Fuse", to = "Comm")
dag1 <- set.arc (dag1, from = "Plea", to = "Comm")
dag1 <- set.arc (dag1, from = "Atti", to = "Comm")

# Visualize DAG
plot(dag1)

# Print info
dag1
```
```{r Method 2}
dag2 <- empty.graph(nodes = c("Fuse","Plea","Atti","Comm"))

arcs(dag2) = matrix (c("Fuse", "Atti",
                       "Plea", "Atti",
                       "Fuse", "Comm",
                       "Plea", "Comm",
                       "Atti", "Comm"),
                     byrow  = TRUE, ncol = 2,
                     dimnames = list (NULL, c("from", "to")))

plot(dag2)
```

```{r Method 3}
# An easy way to build the DAG when we know the structure:
dag3 <- model2network("[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]")
plot(dag3)
```

### Specify probabilities

```{r}
# Specify categories per factor
Fuse.lv <- c("Low", "Med", "High") 
Plea.lv <- c("Low", "Med", "High")
Atti.lv <- c("Low", "Med", "High")
Comm.lv <- c("Low", "Med", "High")

# Probabilities "Fuse": Equal to priors because it has no arrows towards it
Fuse.prob <- array(c(0.02, 0.26, 0.72), dim = 3, dimnames = list(Fuse = Fuse.lv))
Fuse.prob

# Same as for "Fuse"
Plea.prob <- array(c(0.01, 0.55, 0.44), 
                   dim = 3, 
                   dimnames= list (Plea = Plea.lv))
Plea.prob

# "Atti" is conditional on both "Plea" and "Fuse", therefore 3x3x3 probabilities.
Atti.prob <- array(c(0.99, 0.01, 0.00,
                     0.00, 0.67, 0.33,
                     0.01, 0.99, 0.00,
                     0.34, 0.33, 0.33, 
                     0.00, 0.79, 0.21,
                     0.00, 0.40, 0.60,
                     0.99, 0.01, 0.00,
                     0.00, 0.47, 0.53,
                     0.00, 0.09, 0.91), 
                   dim = c(3, 3, 3), 
                   dimnames= list(Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv))
Atti.prob

# "Comm" is conditional on all three variables. Therefore, 3x3x3x3 probabilities.
Comm.prob <- array (c(0.00, 1.00, 0.00, 
                      0.34, 0.33, 0.33, 
                      0.34, 0.33, 0.33, 
                      0.34, 0.33, 0.33, 
                      0.00, 1.00, 0.00, 
                      1.00, 0.00, 0.00, 
                      0.34, 0.33, 0.33,
                      0.00, 1.00, 0.00, 
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.00, 0.98, 0.02,
                      0.00, 0.83, 0.17,
                      0.34, 0.33, 0.33,
                      0.00, 0.33, 0.67,
                      0.00, 0.44, 0.56,
                      1.00, 0.00, 0.00,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.00, 0.84, 0.16,
                      0.00, 0.71, 0.29,
                      0.34, 0.33, 0.33,
                      0.00, 0.40, 0.60,
                      0.00, 0.10, 0.90), 
                    dim = c (3, 3, 3, 3), 
                    dimnames= list(Comm = Comm.lv,  Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv))
Comm.prob
```
### Build model

```{r}
# Relate the CPT to the labels
cpt <- list(Fuse = Fuse.prob, 
            Plea = Plea.prob,
            Atti = Atti.prob, 
            Comm = Comm.prob)

#  Relate the DAG and CPT and define a fully-specified Bayesian Network
bn <- custom.fit(dag, cpt)

# Print model
bn
```

## Learn from data

We can use two different classes of algorithms to estimate the model:
- Constraint-based algorithms
- Score-based algorithms

### Constraint-based algorithms

Here, we can choose between different algorithms:
- Grow-Shrink (gs)
- Incremental Association (iamb)
- Fast Incremental Association (fast.iamb)
- Interleaved Incremental Association (inter.iamb)

We can also use pass different values to the "test" argument:
- test = "x2"
- test = "mi"

Does it matter? Idk

```{r}
bn.gs <- gs(retention, alpha = 0.05, test ="x2")
bn.iamb <- iamb(retention, alpha = 0.05, test ="x2")
bn.fast.iamb <- fast.iamb (retention, alpha = 0.05, test ="mi")
bn.inter.iamb <- inter.iamb (retention, alpha = 0.05, test ="mi" )

graphviz.plot(bn.gs, main = "GrowShrink_X2")
graphviz.plot(bn.fast.iamb, main = "FastIAMB_MI")
```

#### Detect undirected arc

In the manually specified model we know the directed arcs between the variables.
They are probably based on some theoretical foundation.

When learning DAGs from data, we cannot know if the algorithm has found the same arcs.

In the case below, the algorithm did not find the arc between "Atti" and "Comm".

```{r}
undirected.arcs(bn.gs)
```

Since we need direction for all graphs in a DAG, we add it for this connection:

```{r}
bn.gs1 <- set.arc(bn.gs, from = "Atti", to = "Comm")
graphviz.plot(bn.gs1, main = "GrowShrink_X2_2")
```

### Score-based algorithms

We can choose between different score-based algorithms:
- Hill-Climbing greedy search (hc)
- ?

Common for all of them is that we choose the model that maximizes a score. 
Pass the score we want to maximise on to the function using the "score" argument.

Scores:
- BIC 
- AIC
- loglik
- Run "?bnlearn::`network-scores`" to see more

```{r}
bn.hc <- hc(retention, score = "bic")
graphviz.plot(bn.hc, main = "Hill Climbing - BIC") 
```

### Fit model

```{r}
bn.mle <- bn.fit (bn.gs1, data = retention, method = "mle")
bn.mle

# print (conditional) probabilities
bn.mle$Fuse
bn.mle$Plea
bn.mle$Atti
bn.mle$Comm
```
## Other useful functions

```{r}
# Drop arc
bn.gs2 <- drop.arc(bn.gs1, from="Atti", to="Comm")
plot(bn.gs2)

# Test for the conditional independence between variables 
graphviz.plot(bn.gs1)
ci.test("Atti", "Comm", c("Fuse", "Plea"), test = "x2", data = retention)

# Get Markov blanket for a given variable
mb(bn.gs1, "Atti")
```

# Model evaluation

## Model complexity

```{r}
nodes(bn.mle)
arcs(bn.mle)
bn.mle
```

## Model sensitivity

```{r}
dsep(bn.mle, x = "Plea", y = "Fuse")
dsep(bn.mle, x = "Plea", y = "Comm")
```

## Arc strength

a) with criterion ="x2" or "mi", the output reports the p-value for the test. 
   The lower the p-value, the stronger the relationship. 
   
```{r}
arc.strength (bn.gs1, retention, criterion = "x2") %>%.[order(.$strength),]
```
b) with criterion ="bic" reports the change in the BIC score of the net caused 
   by an arc removal.The more negative the change, means the BIC score will go 
   worse if we delete that arc (i.e. the arc is important for the model).

Previously, we chose between:
- Constraint-based algorithms
- Score-based algorithms

If the model is built using a score-based algorithm with score = "bic" then we will most likely see a negative consequence of arc removal.

```{r}
# Repeating the analysis for the hill-climbing structure
arc.strength (bn.hc, retention, criterion = "bic") %>%.[order(.$strength),]
```

If we do it for the constraint-based models we see a different pattern
The output reveals that, if we remove Atti -> Comm, BIC will increase with 40.48, 
which in bnlearn package means the model may improve based on this index.

```{r}
arc.strength (bn.gs1, retention, criterion = "bic") %>%.[order(.$strength),]
```

## Model comparison

BIC, BDe, AIC scores are used to compare alternative structures and choose the best  
In bnlearn, AIC, BIC, BDE closer to zero means better model; often the three indexes
do not agree.

```{r}
# AIC
bnlearn::score (bn.gs1, retention, type = "aic")
bnlearn::score (bn.hc, retention, type = "aic")

# BIC
bnlearn::score (bn.gs1, retention, type = "bic")
bnlearn::score (bn.hc, retention, type = "bic")
```

# Predictive accuracy

## K-fold cross-validation

This function requires as one of its parameters only structure, not the full model
Here I use classification error ("pred") for the node Comm (our target) as a loss function. 

The prediction accuracy of Comm based on 5-fold cross validation is 1-0.18 = 0.82 
In a similar way one can assess each individual variable.

```{r}
netcv = bn.cv(retention, bn.gs1, loss ="pred", k = 5, loss.args = list(target = "Comm"), debug = TRUE)
netcv 
```

## Using a testing sample

Here we rely on the gRain package.
First transform the network into a gRain object, then make predictions -> confusion matrix.
We again measure the model's ability to predict "Comm". Could repeat for the others as well.

```{r}
net1 <- as.grain(bn.mle)

# Get probability predictions
predComm <- predict(net1, response = c("Comm"), newdata = retention_test, 
                    predictors = names (retention_test)[-4], # Comm is 4th col in test df
                    type = "distribution") 

predComm <- predComm$pred$Comm
head(predComm, 5)

# Get class predictions
predComm_class = predict (net1, response = c("Comm"), 
                          newdata = retention_test, 
                          predictors = names (retention_test)[-4], 
                          type = "class")
predCommclass = predComm_class$pred$Comm
head(predCommclass, 5)

########################################################################
# Another method if you cannot use package gRain 
bn.mle1 = bn.fit(model2network("[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]"),retention) 
predComm1= predict(bn.mle1, node = "Comm",data = retention_test) 
predComm1
table(predCommclass, predComm1)
########################################################################

# True values
table(retention_test$Comm)

# Confusion matrix
table(predComm_class$pred$Comm, retention_test$Comm)
```
#### AUC
- Requires the predicted probabilities, not the predicted class
- We get an AUC for every column of the prediction matrix
- Our DV has 3 categories: Low, Med and High
- We observe that the model has problems when distinguishing between high and medium
  but performs pretty well when identifying the Low category (customers who are not committed to VC)
  
```{r}
colAUC(predComm, retention_test[ ,4], plotROC = TRUE)
```

# Making queries

```{r}
# Transform the bn into a junction tree 
# options(digits=1)
junction <- compile(as.grain(bn.mle))

# "querygrain" function extracts the marginal distribution of the nodes
querygrain(junction, nodes = "Atti")
querygrain(junction, nodes = "Comm")

# if Fuse = Low
jLow <- setEvidence(junction, nodes = "Fuse", states = "Low")
A1 = querygrain(jLow, nodes = "Atti")
A1
C1= querygrain(jLow, nodes = "Comm")
C1

# if Fuse = Med
jMed <- setEvidence(junction, nodes = "Fuse", states = "Med")
A2 = querygrain(jMed, nodes = "Atti")
A2
C2 = querygrain(jMed, nodes = "Comm")
C2


# if Fuse = High
jHigh <- setEvidence (junction, nodes = "Fuse", states = "High")
A3 = querygrain(jHigh, nodes = "Atti")
A3
C3 = querygrain(jHigh, nodes = "Comm")
C3

# Summary (only for Atti)
AttiHigh <- c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]])
AttiLow <- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]])
AttiMed <-c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]])


df1 <- data.frame(Fuse = c("Low", "Med", "High"), AttiLow, AttiMed, AttiHigh)
df1
matplot(rownames(df1), df1, type='l', xlab='Fuse', ylab='', ylim=c(0,1))
legend('topright', inset=.01, legend=colnames(df1[,2:4]), 
       pch=1, horiz=T, col=2:4)
```
## Discussion
As Fuse changes from Low to Medium to High, 
   - the high state of attitude shows an increasing trend, 
   - the medium state of attitude shows a decreasing trend,
   - the low state of attitude shows a constant trend. 
Notice in the figure that when functional usefulness is low (left-side), 
the probability of attitude medium is quite high (0.80); it may suggest that
functional usefulness does not radically affect the customer´s attitude. 

# Targeted Ads

I Targetd Ads scriptet bruges en anden metode, skal den uddybes?

```{r}
targeted.adv.beta <- read.csv("../Code/Data/simulated_targeted_adv_data.csv", header = T, colClasses = "factor")

# Build the structure
nb_structure <- tree.bayes(targeted.adv.beta[, -1], "Buy")
plot(nb_structure)

# Learn the parameters
bnTA.mle <- bn.fit (nb_structure, data = targeted.adv.beta[, -1], method = "mle")
bnTA.mle

junctionTA <- compile (as.grain(bnTA.mle))
Query_yes <- setEvidence (junctionTA, nodes = c("Marital.Status", 
                                                "Device.Usage",
                                                   "Age", 
                                                  "Mailed"), 
                             states = c("Married", 
                                        "Desktop",
                                        "33-54",
                                        "Yes"))
querygrain(Query_yes, nodes = "Buy")
```

# Product Recommendation

Her bruges også andre funktioner.
Laver en latent class analysis.

```{r}
library(poLCA)      # for latent classification 
library(bnlearn)    # for building BN
library (gRain)     # for querying BN
library(Rgraphviz)  # for visualizing BN

# Maybe insert code

```

