---
title: "03 Bayesian Networks"
author: "Morten Gade"
date: "2024-05-30"
output: html_document
---

# Packages and data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 0)
library(dplyr)
library(bnlearn)
library(Rgraphviz)
library(gRain)
library(gRbase)
library (caTools)
library(poLCA)
retention <- read.csv("data/retention.csv", header = T, colClasses = "factor" )
retention_test <- read.csv("data/retention_test.csv", header = T, colClasses = "factor" )

```

# Creating DAGs

DAG: Directed Acyclical Graph
- No loops
- Direction: Causality

We go through two methods for creating DAGs here:
1. Building the structure manually and introducing probabilities manually 
2. Learning the structure and the probabilities from data - most common

## Build manually

First, we define the DAG.
There are several ways of doing this, I have included 3 options below.
Then, we need to specify probabilities and conditional probabilities.

### Build DAG

```{r Method 1}
# Create an empty graph 
dag1 <- empty.graph(nodes = c("Fuse","Plea","Atti","Comm"))

# Add the arcs that encode the direct dependencies between variables
dag1 <- set.arc (dag1, from = "Fuse", to = "Atti")
dag1 <- set.arc (dag1, from = "Plea", to = "Atti")
dag1 <- set.arc (dag1, from = "Fuse", to = "Comm")
dag1 <- set.arc (dag1, from = "Plea", to = "Comm")
dag1 <- set.arc (dag1, from = "Atti", to = "Comm")

# Visualize DAG
plot(dag1)

# Print info
dag1
```
```{r Method 2}
dag2 <- empty.graph(nodes = c("Fuse","Plea","Atti","Comm"))

arcs(dag2) = matrix (c("Fuse", "Atti",
                       "Plea", "Atti",
                       "Fuse", "Comm",
                       "Plea", "Comm",
                       "Atti", "Comm"),
                     byrow  = TRUE, ncol = 2,
                     dimnames = list (NULL, c("from", "to")))

plot(dag2)
```

```{r Method 3}
# An easy way to build the DAG when we know the structure:
dag3 <- model2network("[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]")
plot(dag3)
```

### Specify probabilities

```{r}
# Specify categories per factor
Fuse.lv <- c("Low", "Med", "High") 
Plea.lv <- c("Low", "Med", "High")
Atti.lv <- c("Low", "Med", "High")
Comm.lv <- c("Low", "Med", "High")

# Probabilities "Fuse": Equal to priors because it has no arrows towards it
Fuse.prob <- array(c(0.02, 0.26, 0.72), dim = 3, dimnames = list(Fuse = Fuse.lv))
Fuse.prob

# Same as for "Fuse"
Plea.prob <- array(c(0.01, 0.55, 0.44), 
                   dim = 3, 
                   dimnames= list (Plea = Plea.lv))
Plea.prob

# "Atti" is conditional on both "Plea" and "Fuse", therefore 3x3x3 probabilities.
Atti.prob <- array(c(0.99, 0.01, 0.00,
                     0.00, 0.67, 0.33,
                     0.01, 0.99, 0.00,
                     0.34, 0.33, 0.33, 
                     0.00, 0.79, 0.21,
                     0.00, 0.40, 0.60,
                     0.99, 0.01, 0.00,
                     0.00, 0.47, 0.53,
                     0.00, 0.09, 0.91), 
                   dim = c(3, 3, 3), 
                   dimnames= list(Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv))
Atti.prob

# "Comm" is conditional on all three variables. Therefore, 3x3x3x3 probabilities.
Comm.prob <- array (c(0.00, 1.00, 0.00, 
                      0.34, 0.33, 0.33, 
                      0.34, 0.33, 0.33, 
                      0.34, 0.33, 0.33, 
                      0.00, 1.00, 0.00, 
                      1.00, 0.00, 0.00, 
                      0.34, 0.33, 0.33,
                      0.00, 1.00, 0.00, 
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.00, 0.98, 0.02,
                      0.00, 0.83, 0.17,
                      0.34, 0.33, 0.33,
                      0.00, 0.33, 0.67,
                      0.00, 0.44, 0.56,
                      1.00, 0.00, 0.00,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.34, 0.33, 0.33,
                      0.00, 0.84, 0.16,
                      0.00, 0.71, 0.29,
                      0.34, 0.33, 0.33,
                      0.00, 0.40, 0.60,
                      0.00, 0.10, 0.90), 
                    dim = c (3, 3, 3, 3), 
                    dimnames= list(Comm = Comm.lv,  Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv))
Comm.prob
# Interpreting the first conditional probability table:
# , , Plea = Low, Fuse = Low
#
#       Atti
# Comm   Low  Med High
#   Low    0 0.34 0.34
#   Med    1 0.33 0.33
#   High   0 0.33 0.33

# Given that Plea, Fuse is low, the probability Comm is medium given that Atti is low is 1
```
### Build model

```{r}
# Relate the CPT to the labels
cpt <- list(Fuse = Fuse.prob, 
            Plea = Plea.prob,
            Atti = Atti.prob, 
            Comm = Comm.prob)

#  Relate the DAG and CPT and define a fully-specified Bayesian Network
bn <- custom.fit(dag1, cpt)

# Print model
bn

# Fuse and Plea is independent variables and therefore not influenced by other variables. (Simple CPTs)
```

## Learn from data

We can use two different classes of algorithms to estimate the model:
- Constraint-based algorithms
- Score-based algorithms

### Constraint-based algorithms

Here, we can choose between different algorithms:
- Grow-Shrink (gs)
- Incremental Association (iamb)
- Fast Incremental Association (fast.iamb)
- Interleaved Incremental Association (inter.iamb)

We can also use pass different statistics to the "test" argument:
- test = "x2"
- test = "mi"

Does it matter? Idk

```{r}
# We estimate the structure of the model using the four approaches
bn.gs <- gs(retention, alpha = 0.05, test ="x2")
bn.iamb <- iamb(retention, alpha = 0.05, test ="x2")
bn.fast.iamb <- fast.iamb (retention, alpha = 0.05, test ="mi")
bn.inter.iamb <- inter.iamb (retention, alpha = 0.05, test ="mi" )

graphviz.plot(bn.gs, main = "GrowShrink_X2")
graphviz.plot(bn.fast.iamb, main = "FastIAMB_MI")
```

#### Detect undirected arc

In the manually specified model we know the directed arcs between the variables.
They are probably based on some theoretical foundation.

When learning DAGs from data, we cannot know if the algorithm has found the same arcs.

In the case below, the algorithm did not find the arc between "Atti" and "Comm".

```{r}
undirected.arcs(bn.gs)
```

Since we need direction for all graphs in a DAG, we add it for this connection:

```{r}
bn.gs1 <- set.arc(bn.gs, from = "Atti", to = "Comm")
graphviz.plot(bn.gs1, main = "GrowShrink_X2_2")
```

### Score-based algorithms

We can choose between different score-based algorithms:
- Hill-Climbing greedy search (hc)
- ?

Common for all of them is that we choose the model that maximizes a score. 
Pass the score we want to maximise on to the function using the "score" argument.

Scores:
- BIC 
- AIC
- loglik
- Run "?bnlearn::`network-scores`" to see more

```{r}
bn.hc <- hc(retention, score = "bic")
graphviz.plot(bn.hc, main = "Hill Climbing - BIC") 
```

### Fit model

```{r}
# We previously tested four algorithms for estimating the structure of the model. After picking one of them
# we proceed to estimate the probabilities and fit them to the model.
bn.mle <- bn.fit (bn.gs1, data = retention, method = "mle")
bn.mle

# print (conditional) probabilities
bn.mle$Fuse
bn.mle$Plea
bn.mle$Atti
bn.mle$Comm
```
## Other useful functions

```{r}
# Drop arc
bn.gs2 <- drop.arc(bn.gs1, from="Atti", to="Comm")
plot(bn.gs2)

# Test for the conditional independence between variables. In this case we test if there is 
# a significant influence from atti to comm given we control for the influence of fuse and plea.
# If the P-value is < 0.05, Atti do influence Comm even if we control for Fuse + Plea.
graphviz.plot(bn.gs1)
ci.test("Atti", "Comm", c("Fuse", "Plea"), test = "x2", data = retention)

# Get Markov blanket for a given variable
mb(bn.gs1, "Atti")
```

# Model evaluation

## Model complexity

```{r}
nodes(bn.mle)
arcs(bn.mle)
bn.mle
```

## Model sensitivity

```{r}
# Test if there is a relation between variables. (If there is an arrow in the structure)
dsep(bn.mle, x = "Plea", y = "Fuse")
dsep(bn.mle, x = "Plea", y = "Comm")
```

## Arc strength
How significant is the relation between two variables?

a) with criterion ="x2" or "mi", the output reports the p-value for the test. 
   The lower the p-value, the stronger the relationship. 
   
```{r}
arc.strength (bn.gs1, retention, criterion = "x2") %>%.[order(.$strength),]
```
b) with criterion ="bic" reports the change in the BIC score of the net caused 
   by an arc removal.The more negative the change, means the BIC score will go 
   worse if we delete that arc (i.e. the arc is important for the model).

Previously, we chose between:
- Constraint-based algorithms
- Score-based algorithms

If the model is built using a score-based algorithm with score = "bic" then we will most likely see a negative consequence of arc removal.

```{r}
# Repeating the analysis for the hill-climbing structure
arc.strength (bn.hc, retention, criterion = "bic") %>%.[order(.$strength),]
```

If we do it for the constraint-based models we see a different pattern
The output reveals that, if we remove Atti -> Comm, BIC will increase with 40.48, 
which in bnlearn package means the model may improve based on this index.

```{r}
arc.strength (bn.gs1, retention, criterion = "bic") %>%.[order(.$strength),]
```

## Model comparison

BIC, BDe, AIC scores are used to compare alternative structures and choose the best  
In bnlearn, AIC, BIC, BDE the lower the score, the better the model; often the three indexes
do not agree.

```{r}
# AIC
bnlearn::score (bn.gs1, retention, type = "aic")
bnlearn::score (bn.hc, retention, type = "aic")

# BIC
bnlearn::score (bn.gs1, retention, type = "bic")
bnlearn::score (bn.hc, retention, type = "bic")

# BDE
bnlearn::score (bn.gs1, retention, type = "bde")
bnlearn::score (bn.hc, retention, type = "bde")
```

# Predictive accuracy

## K-fold cross-validation

This function requires as one of its parameters only structure, not the full model
Here I use classification error ("pred") for the node Comm (our target) as a loss function. 

The prediction accuracy of Comm based on 5-fold cross validation is 1-0.18 = 0.82 
In a similar way one can assess each individual variable.

```{r}
netcv = bn.cv(retention, bn.gs1, loss ="pred", k = 5, loss.args = list(target = "Comm"), debug = TRUE)
netcv
# Based on expected loss, we calculate the accuracy for predicting Comm as 1 - expected loss (bottom row of output)
```

## Using a testing sample

Here we rely on the gRain package.
First transform the network into a gRain object, then make predictions -> confusion matrix.
We again measure the model's ability to predict "Comm". Could repeat for the others as well.

```{r}
net1 <- as.grain(bn.mle) # Refers to the model built in the 'Fit model' section.

# Get probability predictions
predComm <- predict(net1, response = c("Comm"), newdata = retention_test, 
                    predictors = names (retention_test)[-4], # Comm is 4th col in test df
                    type = "distribution") 

predComm <- predComm$pred$Comm
head(predComm, 5)

# Get class predictions
predComm_class = predict (net1, response = c("Comm"), 
                          newdata = retention_test, 
                          predictors = names (retention_test)[-4], 
                          type = "class")
predCommclass = predComm_class$pred$Comm
head(predCommclass, 5)

########################################################################
# Another method if you cannot use package gRain 
bn.mle1 = bn.fit(model2network("[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]"),retention) 
predComm1= predict(bn.mle1, node = "Comm",data = retention_test) 
predComm1
table(predCommclass, predComm1)
# Rows: estimated. Columns: Reference
########################################################################

# True values
table(retention_test$Comm)

# Confusion matrix
table(predComm_class$pred$Comm, retention_test$Comm)
```
#### AUC
- Requires the predicted probabilities, not the predicted class
- We get an AUC for every column of the prediction matrix
- Our DV has 3 categories: Low, Med and High
- We observe that the model has problems when distinguishing between high and medium
  but performs pretty well when identifying the Low category (customers who are not committed to VC)
  
```{r}
colAUC(predComm, retention_test[ ,4], plotROC = TRUE)
# The black line "High [high vs. low]: When the variable is High, the model easily reaches a high sensitivity, yielding a high AUC. This is due to the reason that it easily distinguishes between high and low when the variable is high.
# The light blue line Low [High vs. Med]: When the variable is Low, the model struggles to reach high sensitivity, yielding a poor distinguish between high and medium. That means poor prediction.
```

# Making queries

```{r}
# In this section we want to investigate how the two variables, Atti and Comp change as we define the level of Fuse

# Transform the bn into a junction tree 
# options(digits=1)
junction <- compile(as.grain(bn.mle))

# "querygrain" function extracts the marginal distribution of the nodes
querygrain(junction, nodes = "Atti")
querygrain(junction, nodes = "Comm")

# if Fuse = Low
jLow <- setEvidence(junction, nodes = "Fuse", states = "Low")
A1 = querygrain(jLow, nodes = "Atti")
A1
C1= querygrain(jLow, nodes = "Comm")
C1

# if Fuse = Med
jMed <- setEvidence(junction, nodes = "Fuse", states = "Med")
A2 = querygrain(jMed, nodes = "Atti")
A2
C2 = querygrain(jMed, nodes = "Comm")
C2


# if Fuse = High
jHigh <- setEvidence (junction, nodes = "Fuse", states = "High")
A3 = querygrain(jHigh, nodes = "Atti")
A3
C3 = querygrain(jHigh, nodes = "Comm")
C3

# Summary (only for Atti)
AttiHigh <- c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]])
AttiLow <- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]])
AttiMed <-c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]])


df1 <- data.frame(Fuse = c("Low", "Med", "High"), AttiLow, AttiMed, AttiHigh)
df1
matplot(rownames(df1), df1, type='l', xlab='Fuse', ylab='', ylim=c(0,1))
legend('topright', inset=.01, legend=colnames(df1[,2:4]), 
       pch=1, horiz=T, col=2:4)
```
## Discussion
As Fuse changes from Low to Medium to High, 
   - the high state of attitude shows an increasing trend, 
   - the medium state of attitude shows a decreasing trend,
   - the low state of attitude shows a constant trend. 
Notice in the figure that when functional usefulness is low (left-side), 
the probability of attitude medium is quite high (0.80); it may suggest that
functional usefulness does not radically affect the customer´s attitude. 

# Targeted Ads

I Targetd Ads scriptet bruges en anden metode, skal den uddybes?

```{r}
targeted.adv.beta <- read.csv("data/simulated_targeted_adv_data.csv", header = T, colClasses = "factor")

# Build the structure
nb_structure <- tree.bayes(targeted.adv.beta[, -1], "Buy")
plot(nb_structure)

# Learn the parameters
bnTA.mle <- bn.fit (nb_structure, data = targeted.adv.beta[, -1], method = "mle")
bnTA.mle

# Asumming 
c=0.5
r_s = 8
r_u = 10

# a. ) Compute the ELP for the population consisting of individuals with the 
# following characteristics: 


# Married, using desktop, average age
library (gRain)
junctionTA <- compile (as.grain(bnTA.mle))
Query_yes <- setEvidence (junctionTA, nodes = c("Marital.Status", 
                                                "Device.Usage",
                                                   "Age", 
                                                  "Mailed"), 
                             states = c("Married", 
                                        "Desktop",
                                        "33-54",
                                        "Yes"))
querygrain(Query_yes, nodes = "Buy")


Query_no<- setEvidence (junctionTA, nodes = c("Marital.Status", 
                                                "Device.Usage",
                                                "Age", 
                                                "Mailed"), 
                          states = c("Married", 
                                     "Desktop",
                                     "33-54",
                                     "No"))
querygrain(Query_no, nodes = "Buy")

options(digits=2)
ELP = querygrain(Query_yes, nodes = "Buy")$Buy[[2]] * r_s -
  querygrain(Query_no, nodes = "Buy")$Buy[[2]] * r_u - c
ELP
# Since the ELP is positive and high, we mail to this population
```

# Product Recommendation

Here we use cluster learning. We use latent class analysis to create segments and use segment membership to make product recommendations. 

```{r}
library(poLCA)      # for latent classification 
library(bnlearn)    # for building BN
library (gRain)     # for querying BN
library(Rgraphviz)  # for visualizing BN

# Data on user preferences
products = read.csv("Data/CollFilforR.csv", header = T, colClasses = "factor", sep = ";")
View(products)
```

```{r}
set.seed(234)
# Defining the variables used in the model 
f <- cbind(V1, V2, V3, V4)~1 #(~1 means without covariates)

# Then we find the optimal number of classes evaluating on BIC and save the best model
bic_values <- numeric(length = 5) #we are testing 5 models (from 2 to 6 classes)
min_bic <- 100000
for(i in 2:6){
  lc <- poLCA(f, products, nclass=i, maxiter=3000, 
              tol=1e-5, na.rm=FALSE,  
              nrep=1, verbose=TRUE, calc.se=TRUE)
  
  bic_values[i-1] <- lc$bic  # Store the BIC value for each model
  
  if(lc$bic < min_bic){
    min_bic <- lc$bic
    LCA_best_model<-lc
  }
} 
```

```{r}
# bic
print(bic_values)
# plot bic
classes <- 2:6
plot(classes,
     bic_values, 
     type = "b", 
     pch = 19, xlab = "Number of Classes", ylab = "BIC",
     main = "BIC Values by Number of Classes")

# best selected model - full output
LCA_best_model 
# The best model only has two classes and the probability can be found below

# specific output
LCA_best_model$posterior # matrix of posterior class membership probabilities
LCA_best_model$predclass # class membership
LCA_best_model$probs # estimated class-conditional response probabilities

# We add it to the data frame
# save the class
products_copy <- products
products_copy$class <- factor(LCA_best_model$predclass)
```

```{r}
# Second step, for making product recommendations, 
# set up a BN classifier considering the learned class as the 'root' node 
# and preferences as ´children' nodes
products_wNA = na.omit(products_copy) # remove missing data from the dataset
# learn the structure (e.g., TAN structure)
# We specify class as the root node which determines the prduct score within each class 
products_dag = tree.bayes(products_wNA, training = "class") 
graphviz.plot(products_dag)
# learn the parameters
products.bn.mle <- bn.fit(products_dag, data = products_wNA, method = "mle")
bn.fit.barchart(products.bn.mle$V1)
```

```{r}
# We set evidence for some of the variables.
library (gRain)
junction <- compile (as.grain(products.bn.mle))

# now we can use the net for inference
V1V2V3 <- setEvidence (junction, nodes = c("V1", "V2", "V3"), states = c("5", "1", "1"))
prediction = querygrain(V1V2V3, nodes = "V4")
str(prediction)
prediction$V4
```
```{r}
#    1    2    4    5 
#  0.36 0.64 0.00 0.00 
# this user most likely will score 2 or 1 => do not recommend the product V4
# his expected preference for V4 is 
preference = prediction$V4[[1]]*1 + prediction$V4[[2]]*2 + prediction$V4[[3]]*4 + prediction$V4[[4]]*5  
preference
# The predicted score is 1.6

querygrain(V1V2V3, nodes = "class") # to get the expected class 
# The predicted class is 2
```
```{r}
# To make recommendation in a dataset and save them
# Loop through each row in dataset (in this example we use our data)
data=products_copy
# Initialize predictions_df outside the loop
predictions_df <- data.frame(RowIndex = integer(), Node = character(), stringsAsFactors = FALSE)

# Loop through each row in your dataset to predict and save the most probable product to recommend
for (i in 1:nrow(data)) {
  knownPreferences <- !is.na(data[i, ])
  unknownPreferences <- is.na(data[i, ])
  
  # Ensure there is at least one known preference to set as evidence
  if(any(knownPreferences)) {
    nodes <- names(data)[knownPreferences]
    states <- as.character(data[i, knownPreferences])
    
    # Only proceed if there are nodes and states to set as evidence
    if(length(nodes) > 0 && length(states) > 0) {
      junctionWithEvidence <- setEvidence(junction, nodes = nodes, states = states)
      
      
      # Now, proceed to query the model for unknown preferences
      for (nodeToPredict in names(data)[unknownPreferences]) {
        prediction <- querygrain(junctionWithEvidence, nodes = nodeToPredict)
        predictions_df <- rbind(predictions_df, data.frame(RowIndex = i, Node = nodeToPredict, stringsAsFactors = FALSE))
      }
      }
    }
  }

View(predictions_df)
write.csv(predictions_df, "predictions_product.csv", row.names = FALSE)


# To save the probabilities instead of the most probable state
predictions_df <- data.frame(RowIndex = integer(),
                             Node = character(),
                             State = character(),
                             Probability = numeric(),
                             stringsAsFactors = FALSE)

for (i in 1:nrow(data)) {
  knownPreferences <- !is.na(data[i, ])
  unknownPreferences <- is.na(data[i, ])
  
  if(any(knownPreferences)) {
    nodes <- names(data)[knownPreferences]
    states <- as.character(data[i, knownPreferences])
    
    if(length(nodes) > 0 && length(states) > 0) {
      junctionWithEvidence <- setEvidence(junction, nodes = nodes, states = states)
      
      for (nodeToPredict in names(data)[unknownPreferences]) {
        prediction <- querygrain(junctionWithEvidence, nodes = nodeToPredict)
        
       
          # Extract probabilities and their corresponding state names
          probs <- prediction[[nodeToPredict]]  # Accessing the numeric vector of probabilities
          stateNames <- attr(probs, "dimnames")[[1]]  # Extracting state names from dimnames
          
          # Iterate through each state and its probability
          for (j in seq_along(probs)) {
            new_row <- data.frame(RowIndex = i,
                                  Node = nodeToPredict,
                                  State = stateNames[j],
                                  Probability = probs[j],
                                  stringsAsFactors = FALSE)
            predictions_df <- rbind(predictions_df, new_row)
          }
        }
        
      }
    }
  }

View(predictions_df)
write.csv(predictions_df, "prediction_probabilities.csv", row.names = FALSE)
```


 
 